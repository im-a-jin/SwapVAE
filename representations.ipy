import os
import sys
import numpy
import warnings

from absl import app
from absl import flags
import torch
from torch.utils.data import DataLoader

import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger

from neural_kits.dataset import ReachNeuralDataset, get_angular_data, LocalGlobalGenerator
import neural_kits.utils as utils
import neural_kits.transforms as transforms

from vae_kits.dataloaders import spatial_only_neural
from vae_kits.model import swapVAE_neural
from vae_kits.trainers import swap_VAE_neural_Learner

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
if not sys.warnoptions:
    warnings.simplefilter("ignore")

numpy.set_printoptions(threshold=sys.maxsize)
torch.set_printoptions(edgeitems=10000)
# define the global parameters below
FLAGS = flags.FLAGS

# Dataset
flags.DEFINE_string('data_path', './data/mihi-chewie', 'Path to monkey data.')
flags.DEFINE_enum('primate', 'chewie', ['chewie', 'mihi'], 'Primate name.')
flags.DEFINE_integer('day', 1, 'Day of recording.', lower_bound=1, upper_bound=2)
flags.DEFINE_float('binning', 0.1, 'binning_period', lower_bound=0.001, upper_bound=1.)

# Transforms
flags.DEFINE_integer('max_lookahead', 5, 'Max lookahead.')

flags.DEFINE_float('dropout_p', 0.6, 'Dropout probability.', lower_bound=0., upper_bound=1.)
flags.DEFINE_float('dropout_apply_p', 1.0, 'Probability of applying dropout.', lower_bound=0., upper_bound=1.)

flags.DEFINE_float('pepper_p', 0.5, 'Pepper probability.', lower_bound=0., upper_bound=1.)
flags.DEFINE_float('pepper_sigma', 10, 'Pepper sigma.', lower_bound=0.)
flags.DEFINE_float('pepper_apply_p', 1.0, 'Probability of applying pepper.', lower_bound=0., upper_bound=1.)
flags.DEFINE_boolean('structured_transform', True, 'Whether the transformations are consistent across temporal shift.')

# Dataloader
flags.DEFINE_integer('batch_size', 256, 'Batch size.')
flags.DEFINE_integer('num_workers', 1, 'Number of workers.')

# architecture
flags.DEFINE_integer('s_size', 64, 'Style vector size.')
flags.DEFINE_integer('l_size', 128, 'Representation size.')
flags.DEFINE_float('alpha', 10, 'kl loss weight')
flags.DEFINE_float('beta', 1, 'l2 loss weight.')

flags.DEFINE_integer('ablation_type', 0, 'ablation experiment type, 0 is swap default.')

# Training parameters
flags.DEFINE_float('lr', 5e-4, 'Base learning rate.')
flags.DEFINE_integer('num_epochs', 225, 'Number of training epochs.')
flags.DEFINE_integer('check_clf', 5, 'How many epochs to check clf performance.')

# Random seed
flags.DEFINE_integer('random_seed', 100, 'Random seed.')

# Logfile
flags.DEFINE_string('TB_logs', 'test', 'checkpoint and log file name.')
flags.DEFINE_string('TB_logs_folder', 'SwapVAE', 'Tensorboard log folder name.')
from absl import app
sys.argv = sys.argv[:1]
try:
    app.run(lambda argv: None)
except:
    pass

dataset = ReachNeuralDataset(
    FLAGS.data_path,
    primate=FLAGS.primate,
    day=FLAGS.day,
    binning_period=FLAGS.binning,
    scale_firing_rates=False,
    train_split=0.8,
)

transform_temp = transforms.Pair_Compose(
    transforms.RandomizedDropout(FLAGS.dropout_p, apply_p=FLAGS.dropout_apply_p),
)

train_data = spatial_only_neural(dataset, transform=None, target_transform=None, train='train')
test_data = spatial_only_neural(dataset, transform=None, target_transform=None, train='test')

train_data = DataLoader(train_data, batch_size=FLAGS.batch_size, shuffle=True, num_workers=FLAGS.num_workers, drop_last=True)
test_data = DataLoader(test_data, batch_size=FLAGS.batch_size, shuffle=True, num_workers=FLAGS.num_workers, drop_last=True)

train_angular, test_angular = get_angular_data(dataset, device='cuda', velocity_threshold=5)

TB_LOG_NAME = FLAGS.TB_logs
if not os.path.exists("ckpt/{}".format(TB_LOG_NAME)):
    os.makedirs("ckpt/{}".format(TB_LOG_NAME))
logger = TensorBoardLogger(FLAGS.TB_logs_folder, name=TB_LOG_NAME)

number_neurons = next(iter(train_data))[0].shape[1]
model = swapVAE_neural(s_dim=FLAGS.s_size, l_dim=FLAGS.l_size, input_size=number_neurons,
                           hidden_dim=[number_neurons, 128], batchnorm=True)

trainer = pl.Trainer(
        gpus=1, max_epochs=FLAGS.num_epochs,
        accumulate_grad_batches=1,
        # distributed_backend="ddp",
        logger=logger,
    )

learner = swap_VAE_neural_Learner(
    alpha=FLAGS.alpha,
    beta=FLAGS.beta,
    check_clf=FLAGS.check_clf,
    net=model,
    train_angular=train_angular,
    test_angular=test_angular,
    TB_LOG_NAME=TB_LOG_NAME,
    SAVE=20,
    LR=FLAGS.lr,
    l_size=FLAGS.l_size,
    train_mm=train_data,
    test_mm=test_data,
)

model.load_state_dict(torch.load("ckpt/test/epoch219.pth"))
model.eval()
from neural_kits.utils import MetricLogger, batch_iter
temp = batch_iter(*train_angular)
x, a, l, b = next(temp)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = model.to(device)
x = x.to(device)
with torch.no_grad():
    representations = model._representation(x).detach().clone()



# PCA on representations
import matplotlib.pyplot as plt
from cycler import cycler
from sklearn.decomposition import PCA
biter = batch_iter(*train_angular)
x, a, l, b = next(biter)
with torch.no_grad():
    representations = model._representation(x).detach().clone()
representations = representations.cpu()
pca = PCA(n_components=2)
pca.fit(representations)
pca_representations = pca.fit(representations).transform(representations)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
labels = [0, 1, 2, 3, 4, 5, 6, 7]
for i in range(pca_representations.shape[0]):
    label = int(b[i].item())
    plt.scatter(pca_representations[i][0], pca_representations[i][1], color=colors[label], label=labels[label])
leg = plt.legend(['0', '1', '2', '3', '4', '5', '6', '7'])
for i, j in enumerate(leg.legendHandles):
    j.set_color(colors[i])
plt.savefig("pca_representations")



# Train linear classifier
import torch.nn as nn
from tqdm import tqdm
import numpy as np

classifier = nn.Sequential(nn.Linear(model.l_dim, 2))
clf_opt = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)
clf_loss = nn.MSELoss()

classifier.to(device)
classifier.train()
for epoch in tqdm(range(300), disable=True):
    classifier.train()
    for x, _, label, _ in batch_iter(*train_angular, batch_size=FLAGS.batch_size):
        x = x.to(device)
        label = label.to(device)
        clf_opt.zero_grad()
        with torch.no_grad():
            rep = model._representation(x).detach().clone()
    pred = classifier(rep)
    loss = clf_loss(pred, label)
    loss.backward()
    clf_opt.step()

# eval train acc
classifier.eval()
x, a, y, _ = train_angular
with torch.no_grad():
    rep = model._representation(x).detach().clone()
    pred = classifier(rep).detach().clone()
pred_angles = torch.atan2(pred[:, 1], pred[:, 0])
pred_angles[pred_angles < 0] += 2 * np.pi

diff_angles = torch.abs(pred_angles - a.squeeze())
diff_angles[diff_angles > np.pi] -= 2*np.pi

acc = (diff_angles < np.pi/8).sum()
acc = acc.item() / x.size(0)

# eval test acc
classifier.eval()
x, a, y, _ = test_angular
with torch.no_grad():
    rep = model._representation(x).detach().clone()
    pred = classifier(rep).detach().clone()
pred_angles = torch.atan2(pred[:, 1], pred[:, 0])
pred_angles[pred_angles < 0] += 2 * np.pi

diff_angles = torch.abs(pred_angles - a.squeeze())
diff_angles[diff_angles > np.pi] -= 2*np.pi

test_acc = (diff_angles < np.pi/8).sum()
test_acc = test_acc.item() / x.size(0)